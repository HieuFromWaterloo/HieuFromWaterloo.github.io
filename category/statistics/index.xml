<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics | HIEU Q NGUYEN</title>
    <link>https://HieuFromWaterloo.github.io/category/statistics/</link>
      <atom:link href="https://HieuFromWaterloo.github.io/category/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://HieuFromWaterloo.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Statistics</title>
      <link>https://HieuFromWaterloo.github.io/category/statistics/</link>
    </image>
    
    <item>
      <title>Deriving Variation E-M Algorithms on LDA</title>
      <link>https://HieuFromWaterloo.github.io/post/lda/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://HieuFromWaterloo.github.io/post/lda/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;LDA is one of the most important topic models in practice. On a high level, it provides a generative model
that describes how the documents in a dataset are generated; i.e. how words are sampled from multiple
topics to construct a document. This generative process follows a bag of words (BOW) assumption. Hence,
the order in which the word occurs is not taken into account. The core of topic modeling is to analyze unlabeled text data, discover the unknown number of topics and topic distribution in a unsupervised way. This is achieved by making use of statistical inference.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;This note is written based primarily on the following sources:&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;p&gt;D. Blei, Andrew Ng, Jordan M. Latent Dirichlet Allocation. Journal of Machine Learning Research. 2003.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reed C. Latent Dirichlet Allocation: Towards a Deeper Understanding. 2012.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Coursera: Bayesian Methods in Machine Learning. National Research University - Higher School of Economics. 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chenouri S. Stat440/840: Computational Inference. University of Waterloo. 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lysy M. Stat946: Advanced Computational Inference. University of Waterloo. 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Struthers C. Stat450/850: Estimation and Hypothesis Testing. University of Waterloo. 2018.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bishop C. Pattern Recognition and Machine Learning. Information Science and Statistics. 2006.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wolf W. Deriving Expectation-Maximization. Blog. 2018.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Serrano L. Latent Dirichlet Distribution. Youtube. 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
